{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f2b245d",
   "metadata": {},
   "source": [
    "# Tutorial of Aligstein usage\n",
    "\n",
    "In this tutorial we show how to use the Alignstein package by reproducing the biomarkes detection experiment from original paper.\n",
    "\n",
    "Start with importing all needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ab81389",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T15:55:47.701744Z",
     "start_time": "2022-05-23T15:55:46.000909Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Determination of memory status is not supported on this \n",
      " platform, measuring for memoryleaks will never fail\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from Alignstein import (gather_mids, precluster_mids,\n",
    "                        big_clusters_to_clusters, find_consensus_features,\n",
    "                        detect_features_from_file, features_to_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eed5df",
   "metadata": {},
   "source": [
    "We start with obtaining datasets to be analysed. Create `data` directory and download chromatograms from [PRIDE repository](https://www.ebi.ac.uk/pride/archive/projects/PXD013805) as below. It may took some time, thus it is commented.\n",
    "\n",
    "Files enumerated from 38 to 39 represent replicates of experiment for 0 $\\mu$g/L, 40-42 represent 5 $\\mu$g/L, 43-45 represent 50 $\\mu$g/L, 46-48 represent 100 $\\mu$g/L."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdac913c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T15:46:40.994078Z",
     "start_time": "2022-05-23T15:46:40.981539Z"
    }
   },
   "outputs": [],
   "source": [
    "# !mkdir data\n",
    "# !wget https://ftp.ebi.ac.uk/pride-archive/2019/07/PXD013805/20171124VS37.mzML\n",
    "# !wget https://ftp.ebi.ac.uk/pride-archive/2019/07/PXD013805/20171124VS38.mzML\n",
    "# !wget https://ftp.ebi.ac.uk/pride-archive/2019/07/PXD013805/20171124VS39.mzML\n",
    "# !wget https://ftp.ebi.ac.uk/pride-archive/2019/07/PXD013805/20171124VS40.mzML\n",
    "# !wget https://ftp.ebi.ac.uk/pride-archive/2019/07/PXD013805/20171124VS41.mzML\n",
    "# !wget https://ftp.ebi.ac.uk/pride-archive/2019/07/PXD013805/20171124VS42.mzML\n",
    "# !wget https://ftp.ebi.ac.uk/pride-archive/2019/07/PXD013805/20171124VS43.mzML\n",
    "# !wget https://ftp.ebi.ac.uk/pride-archive/2019/07/PXD013805/20171124VS44.mzML\n",
    "# !wget https://ftp.ebi.ac.uk/pride-archive/2019/07/PXD013805/20171124VS42.mzML\n",
    "# !wget https://ftp.ebi.ac.uk/pride-archive/2019/07/PXD013805/20171124VS43.mzML\n",
    "# !wget https://ftp.ebi.ac.uk/pride-archive/2019/07/PXD013805/20171124VS44.mzML\n",
    "# !wget https://ftp.ebi.ac.uk/pride-archive/2019/07/PXD013805/20171124VS45.mzML\n",
    "# !wget https://ftp.ebi.ac.uk/pride-archive/2019/07/PXD013805/20171124VS46.mzML\n",
    "# !wget https://ftp.ebi.ac.uk/pride-archive/2019/07/PXD013805/20171124VS47.mzML\n",
    "# !wget https://ftp.ebi.ac.uk/pride-archive/2019/07/PXD013805/20171124VS48.mzML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456125c9",
   "metadata": {},
   "source": [
    "We perform analysis for replicates of 0 $\\mu$g/L experiment, but it can easily reproduced for the rest experiments.\n",
    "\n",
    "Thus, prepare filenames, which will usable for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c86c2dbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T15:51:43.791846Z",
     "start_time": "2022-05-23T15:51:43.764407Z"
    }
   },
   "outputs": [],
   "source": [
    "filenames = [\n",
    "    \"data/20171124VS37.mzML\",\n",
    "    \"data/20171124VS38.mzML\",\n",
    "    \"data/20171124VS39.mzML\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c357ae94",
   "metadata": {},
   "source": [
    "Detect features in chromatograms. Alignstein uses Feature Finder algorithm from pyOpenMS in centroided mode. It may take some time and logging may be not fully visible in notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e20ecc86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T15:55:50.153027Z",
     "start_time": "2022-05-23T15:55:50.000318Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "the file 'data/20171124VS37.mzML' could not be found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/d0/61cmcxts5h5cxt929wlckcwc0000gn/T/ipykernel_62190/296845542.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfeature_sets_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mfeature_sets_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetect_features_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Generating features may took significant amount of memory not longer used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# It's better to clear cached objects before further run.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projekty/Alignstein/Alignstein/parse.py\u001b[0m in \u001b[0;36mdetect_features_from_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0mIterable\u001b[0m \u001b[0mof\u001b[0m \u001b[0mparsed\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0mrepresented\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mchromatograms\u001b[0m \u001b[0msubsets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \"\"\"\n\u001b[0;32m--> 275\u001b[0;31m     \u001b[0minput_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_chromatogram_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m     \u001b[0mopenms_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     print(\"Parsed file\", filename, \"\\n\", openms_features.size(),\n",
      "\u001b[0;32m~/Documents/Projekty/Alignstein/Alignstein/parse.py\u001b[0m in \u001b[0;36mparse_chromatogram_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mfile_extension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfile_extension\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\".mzml\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparse_ms1_mzml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mfile_extension\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\".mzxml\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mparse_ms1_mzxml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projekty/Alignstein/Alignstein/parse.py\u001b[0m in \u001b[0;36mparse_ms1_mzml\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0minput_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyopenms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSExperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mfh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0minput_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdateRanges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minput_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpyopenms/pyopenms_6.pyx\u001b[0m in \u001b[0;36mpyopenms.pyopenms_6.MzMLFile.load\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: the file 'data/20171124VS37.mzML' could not be found"
     ]
    }
   ],
   "source": [
    "feature_sets_list = []\n",
    "for fname in filenames:\n",
    "    feature_sets_list.append(detect_features_from_file(fname))\n",
    "    # Generating features may took significant amount of memory not longer used\n",
    "    # It's better to clear cached objects before further run.\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac188d21",
   "metadata": {},
   "source": [
    "We scale features' RT by factor proportional (by `SCALE_FACTOR`) to ratio of everage feature width and length. Aim of this scaling is to obatin M/Z axis and RT axis at the same order of magnitude. Thus, further we won't use parameter maximum RT distance below which features are matched. Instead we will talk about maximum feature distance in both dimensions expressed in M/Z order of manitude (Daltons).\n",
    "\n",
    "We scale all datasets by the same factor. This results in different scaling of every dataset, but allows more precise matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fb33634",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T17:20:22.487938Z",
     "start_time": "2022-05-23T17:20:22.468975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [] \n",
      " Average weight nan\n"
     ]
    }
   ],
   "source": [
    "# Scale by average weight\n",
    "SCALE_FACTOR = 5 # Something between 5 and usually work fine but it highly depends on properties of your dataset.\n",
    "\n",
    "weights = [features_to_weight(f_set) for f_set in feature_sets_list]\n",
    "average_weight = np.mean(weights)\n",
    "\n",
    "scale = average_weight * SCALE_FACTOR\n",
    "\n",
    "print(\"Weights:\", weights, \"\\n\", \"Average weight\", average_weight)\n",
    "\n",
    "for feature_set in feature_sets_list:\n",
    "    for feature in feature_set:\n",
    "        feature.scale_rt(scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8bdf29",
   "metadata": {},
   "source": [
    "Start with the first phase - clustering. `gather_mids` function collects feature centroids to be further clustered, then centroid are clustered into several (8-16) areas of by `precluster_mids` function. Finally the main clustering is done by `big_clusters_to_clusters`. This two-step clustering is crucial for proper memory handling.\n",
    "\n",
    "`distance_threshold` parameters controls maximum distance of centroids in one cluster. It is expressed in M/Z order of magnitude (Daltons). The distance is expressed as $\\ell_1$ distance, so it should by about 2 times maximum M/Z variability (to incorporate variability of both M/Z and RT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4f9e183",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T17:21:55.040257Z",
     "start_time": "2022-05-23T17:21:54.798237Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 2)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/d0/61cmcxts5h5cxt929wlckcwc0000gn/T/ipykernel_62190/3487554820.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgather_mids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_sets_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbig_clusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecluster_mids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mclusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbig_clusters_to_clusters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbig_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistance_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projekty/Alignstein/Alignstein/multialign.py\u001b[0m in \u001b[0;36mprecluster_mids\u001b[0;34m(mids)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprecluster_mids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     return np.array(\n\u001b[0;32m--> 127\u001b[0;31m         MiniBatchKMeans(n_clusters=16, init='k-means++', max_iter=100,\n\u001b[0m\u001b[1;32m    128\u001b[0m                         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                         \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_no_improvement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/python39_venv/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36mfit_predict\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mIndex\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcluster\u001b[0m \u001b[0meach\u001b[0m \u001b[0msample\u001b[0m \u001b[0mbelongs\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m         \"\"\"\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/python39_venv/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1892\u001b[0m             \u001b[0mFitted\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m         \"\"\"\n\u001b[0;32m-> 1894\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m   1895\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1896\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/python39_venv/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/python39_venv/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    803\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    806\u001b[0m                 \u001b[0;34m\"Found array with %d sample(s) (shape=%s) while a\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m                 \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 2)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "mids = gather_mids(feature_sets_list)\n",
    "gc.collect()\n",
    "big_clusters = precluster_mids(mids)\n",
    "\n",
    "clusters = big_clusters_to_clusters(mids, big_clusters, distance_threshold=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7d3f20",
   "metadata": {},
   "source": [
    "And finally we do the matching and consensus feature creation. It is done by `find_consensus_features` function. The most important parameters are:\n",
    "- `centroid_upper_bound` which controls the maximum centroid distance for which GWD is computed. For efficiency reasons should be reasonably small, but should not be singnificantly smaller than the maximum distance for which we want to match features;\n",
    "- `gwd_upper_bound` which controls is a parameter of GWD computing (aka. lambda parameter) and allows to omit transporting singal over distance equal to `gwd_upper_bound`. Should be big enough so that the most distant but matchable features are still comparable;\n",
    "- `matching_penalty` - which is penalty for feature not matching. Can be interpreted as maximum distance so that features still should be matched. Above this threshold features are considered as to distant to be matched.\n",
    "- `turns` - in one feature matching not all features may be matched, because features are limited to matched at most one to one cluster. Still, there can be more possible features to be matched which can be matched in next turns. Usually, 2-3 turns are enough, algorithm loops next turn iff there are features which can be matched.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "425b60ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T17:52:22.367651Z",
     "start_time": "2022-05-23T17:52:22.321346Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clusters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/d0/61cmcxts5h5cxt929wlckcwc0000gn/T/ipykernel_62190/355097477.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m consensus_features = find_consensus_features(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mclusters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mfeature_sets_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     centroid_upper_bound=15, gwd_upper_bound=15, matching_penalty=1, turns=10)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clusters' is not defined"
     ]
    }
   ],
   "source": [
    "consensus_features = find_consensus_features(\n",
    "    clusters,\n",
    "    feature_sets_list,\n",
    "    centroid_upper_bound=15, gwd_upper_bound=15, matching_penalty=1, turns=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2d7bd9",
   "metadata": {},
   "source": [
    "Finally dump obained consensus features to `consensus_fetures.out` file with regard to initial features locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a325d8a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T17:54:48.237803Z",
     "start_time": "2022-05-23T17:54:48.176999Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dump_consensus_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/d0/61cmcxts5h5cxt929wlckcwc0000gn/T/ipykernel_62190/620970787.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m dump_consensus_features(consensus_features, \"consensus_fetures.out\",\n\u001b[0m\u001b[1;32m      2\u001b[0m                         feature_sets_list)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dump_consensus_features' is not defined"
     ]
    }
   ],
   "source": [
    "dump_consensus_features(consensus_features, \"consensus_fetures.out\",\n",
    "                        feature_sets_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c658227",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
