{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f2b245d",
   "metadata": {},
   "source": [
    "# Tutorial of Aligstein usage\n",
    "\n",
    "In this tutorial we show how to use the Alignstein package by reproducing the biomarkes detection experiment from original paper.\n",
    "\n",
    "Start with importing all needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab81389",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-23T06:45:36.380684Z",
     "start_time": "2023-01-23T06:45:36.371324Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from Alignstein import (cluster_mids, find_consensus_features,\n",
    "                        detect_features_from_file, features_to_weight,\n",
    "                        parse_chromatogram_with_detected_features,\n",
    "                        dump_consensus_features, dump_consensus_features_caap_style,\n",
    "                        parse_features_from_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eed5df",
   "metadata": {},
   "source": [
    "We start with obtaining datasets to be analysed. Create `data` directory and download chromatograms from [PRIDE repository](https://www.ebi.ac.uk/pride/archive/projects/PXD013805) as below. It may took some time, thus it is commented.\n",
    "\n",
    "Files enumerated from 38 to 39 represent replicates of experiment for $0 \\mu g/L$, 40-42 represent $5 \\mu g/L$, 43-45 represent $50 \\mu g/L$, 46-48 represent $100 \\mu g/L$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdac913c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T23:28:46.602776Z",
     "start_time": "2023-01-22T23:28:46.571839Z"
    }
   },
   "outputs": [],
   "source": [
    "# !mkdir data\n",
    "# !cd data\n",
    "# !wget https://ftp.ebi.ac.uk/pride-archive/2019/07/PXD013805/20171124VS37.mzML\n",
    "# !wget https://ftp.ebi.ac.uk/pride-archive/2019/07/PXD013805/20171124VS38.mzML\n",
    "# !wget https://ftp.ebi.ac.uk/pride-archive/2019/07/PXD013805/20171124VS39.mzML\n",
    "# !wget https://ftp.ebi.ac.uk/pride-archive/2019/07/PXD013805/20171124VS40.mzML\n",
    "# !wget https://ftp.ebi.ac.uk/pride-archive/2019/07/PXD013805/20171124VS41.mzML\n",
    "# !wget https://ftp.ebi.ac.uk/pride-archive/2019/07/PXD013805/20171124VS42.mzML\n",
    "# !wget https://ftp.ebi.ac.uk/pride-archive/2019/07/PXD013805/20171124VS43.mzML\n",
    "# !wget https://ftp.ebi.ac.uk/pride-archive/2019/07/PXD013805/20171124VS44.mzML\n",
    "# !wget https://ftp.ebi.ac.uk/pride-archive/2019/07/PXD013805/20171124VS42.mzML\n",
    "# !wget https://ftp.ebi.ac.uk/pride-archive/2019/07/PXD013805/20171124VS43.mzML\n",
    "# !wget https://ftp.ebi.ac.uk/pride-archive/2019/07/PXD013805/20171124VS44.mzML\n",
    "# !wget https://ftp.ebi.ac.uk/pride-archive/2019/07/PXD013805/20171124VS45.mzML\n",
    "# !wget https://ftp.ebi.ac.uk/pride-archive/2019/07/PXD013805/20171124VS46.mzML\n",
    "# !wget https://ftp.ebi.ac.uk/pride-archive/2019/07/PXD013805/20171124VS47.mzML\n",
    "# !wget https://ftp.ebi.ac.uk/pride-archive/2019/07/PXD013805/20171124VS48.mzML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456125c9",
   "metadata": {},
   "source": [
    "We perform analysis for replicates of $0 \\mu g/L$ experiment, but it can easily reproduced for the rest experiments.\n",
    "\n",
    "Thus, prepare filenames, which will usable for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86c2dbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T23:29:06.217940Z",
     "start_time": "2023-01-22T23:29:06.211501Z"
    }
   },
   "outputs": [],
   "source": [
    "filenames = [\n",
    "    \"data/20171124VS37.mzML\",\n",
    "    \"data/20171124VS38.mzML\",\n",
    "    \"data/20171124VS39.mzML\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c357ae94",
   "metadata": {},
   "source": [
    "Detect features in chromatograms. Alignstein uses Feature Finder algorithm from pyOpenMS in centroided mode. It may take some time and logging may be not fully visible in notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20ecc86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T23:11:05.256302Z",
     "start_time": "2023-01-22T23:09:07.124538Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_sets_list = []\n",
    "for fname in filenames:\n",
    "    feature_sets_list.append(detect_features_from_file(fname))\n",
    "    # Generating features may took significant amount of memory not longer used\n",
    "    # It's better to clear cached objects before further run.\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c3ce8e",
   "metadata": {},
   "source": [
    "Once Alignstein detects features, they are dumped in the same folder as input chromatograms with .featureXML extensions. So next time there is no need for repeating feature detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683e918a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-23T00:18:46.478921Z",
     "start_time": "2023-01-22T23:29:30.975876Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_filenames = [\n",
    "    \"data/20171124VS37.mzML.featureXML\",\n",
    "    \"data/20171124VS38.mzML.featureXML\",\n",
    "    \"data/20171124VS39.mzML.featureXML\"\n",
    "]\n",
    "\n",
    "feature_sets_list = [\n",
    "    parse_chromatogram_with_detected_features(ch_fname, fs_fname)\n",
    "    for ch_fname, fs_fname in zip(filenames, feature_filenames)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac188d21",
   "metadata": {},
   "source": [
    "We scale features' RT by factor proportional to ratio of everage feature width and length (i.e. by `SCALE_FACTOR`). Aim of this scaling is to obatin M/Z axis and RT axis at the same order of magnitude. Thus, further we won't use parameter maximum RT distance below which features are matched. Instead we will talk about maximum feature distance in both dimensions expressed in M/Z order of manitude (Daltons).\n",
    "\n",
    "We scale all datasets by the same factor. This results in different scaling of every dataset, but allows more precise matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb33634",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-23T00:18:47.534170Z",
     "start_time": "2023-01-23T00:18:46.486516Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scale by average weight\n",
    "SCALE_FACTOR = 5 # Something between 5 and 10 usually works fine but it highly\n",
    "                 # depends on properties of your dataset.\n",
    "\n",
    "weights = [features_to_weight(f_set) for f_set in feature_sets_list]\n",
    "average_weight = np.mean(weights)\n",
    "\n",
    "scale = average_weight * SCALE_FACTOR\n",
    "\n",
    "print(\"Weights:\", weights, \"\\n\", \"Average weight\", average_weight)\n",
    "\n",
    "for feature_set in feature_sets_list:\n",
    "    for feature in feature_set:\n",
    "        feature.scale_rt(scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8bdf29",
   "metadata": {},
   "source": [
    "Start with the first phase - clustering. `mids` are feature centroids to be further clustered. Then centroids are clustered in two phases. Firstly, the are clustered into several (8-16) areas (`big_clusters`). Finally the main clustering is done (`clusters`). This two-step clustering is crucial for proper memory handling and further time and space matching optimization and paralelization.\n",
    "\n",
    "`distance_threshold` parameters controls maximum distance of centroids in one cluster. It is expressed in M/Z order of magnitude (Daltons). The distance is expressed as $\\ell_1$ distance, so it should by about 2 times maximum M/Z variability (to incorporate variability of both M/Z and RT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f9e183",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-23T00:18:48.845080Z",
     "start_time": "2023-01-23T00:18:47.537298Z"
    }
   },
   "outputs": [],
   "source": [
    "mids, big_clusters, clusters = cluster_mids(feature_sets_list,\n",
    "                                            distance_threshold=0.4)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7d3f20",
   "metadata": {},
   "source": [
    "And finally we do the matching and consensus feature creation. It is done by `find_consensus_features` function. The most important parameters are:\n",
    "- `centroid_upper_bound` which controls the maximum centroid distance for which GWD is computed. For efficiency reasons should be reasonably small, but should not be singnificantly smaller than the maximum distance for which we want to match features;\n",
    "- `gwd_upper_bound` which controls is a parameter of GWD computing (aka. lambda parameter) and allows to omit transporting singal over distance equal to `gwd_upper_bound`. Should be big enough so that the most distant but matchable features are still comparable;\n",
    "- `matching_penalty` - which is penalty for feature not matching. Can be interpreted as maximum distance so that features still should be matched. Above this threshold features are considered as to distant to be matched.\n",
    "- `turns` - in one feature matching not all features may be matched, because features are limited to matched at most one to one cluster. Still, there can be more possible features to be matched which can be matched in next turns. Usually, 2-3 turns are enough, algorithm loops next turn iff there are features which can be matched.\n",
    "- `monoisotopic_max_ppm` is max difference of monoisotopic MZ-s of features to be matched in ppm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425b60ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-23T00:43:40.427492Z",
     "start_time": "2023-01-23T00:18:48.873823Z"
    }
   },
   "outputs": [],
   "source": [
    "consensus_features = find_consensus_features(\n",
    "    clusters, feature_sets_list, centroid_upper_bound=15, \n",
    "    gwd_upper_bound=15, matching_penalty=1, turns=10,\n",
    "    monoisotopic_max_ppm=15, big_clusters=big_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2d7bd9",
   "metadata": {},
   "source": [
    "Finally dump obained consensus features to `consensus_fetures.out` file with regard to initial features locations. The result file contains rows with consensus features. Single row contains indices for corresponding features in the input chromatograms order. If no feature found then empty space left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a325d8a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-23T06:42:42.502505Z",
     "start_time": "2023-01-23T06:42:42.320679Z"
    }
   },
   "outputs": [],
   "source": [
    "dump_consensus_features(consensus_features, \"consensus_features.out\",\n",
    "                        feature_sets_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152c3104",
   "metadata": {},
   "source": [
    "Features may be also dumped in the other style, similar to that in CAAP evaluation. It requires however, additional parsing of OpenMS features.\n",
    "\n",
    "Known TODO: dump consensus features in OpenMS consensusXML style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28556f4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-23T06:46:14.384308Z",
     "start_time": "2023-01-23T06:46:10.093634Z"
    }
   },
   "outputs": [],
   "source": [
    "openms_features = [parse_features_from_file(filename) for filename in feature_filenames]\n",
    "\n",
    "dump_consensus_features_caap_style(consensus_features, \"consensus_features.out\",\n",
    "                                   feature_sets_list, openms_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
